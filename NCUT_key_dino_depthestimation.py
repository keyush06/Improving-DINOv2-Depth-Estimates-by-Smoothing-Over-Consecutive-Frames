# -*- coding: utf-8 -*-
"""key_DINO_DepthEstimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzXTlaHqWWCMJc1sy4MTTEUKTEECxZCJ
"""

# Copyright (c) Meta Platforms, Inc. and affiliates.

"""# Depth Estimation <a target="_blank" href="https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/depth_estimation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>"""

import sys

INSTALL = False # Switch this to install dependencies
if INSTALL: # Try installing package with extras
    REPO_URL = "https://github.com/facebookresearch/dinov2"
    !{sys.executable} -m pip install -e {REPO_URL}'[extras]' --extra-index-url https://download.pytorch.org/whl/cu117  --extra-index-url https://pypi.nvidia.com
else:
    REPO_PATH = "<FIXME>" # Specify a local path to the repository (or use installed package instead)
    sys.path.append(REPO_PATH)

# Commented out IPython magic to ensure Python compatibility.
# !pip uninstall torch torchvision torchaudio -y
!pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.0 --index-url https://download.pytorch.org/whl/cu117

!git clone https://github.com/facebookresearch/dinov2.git
# %cd dinov2
!pip install -e .

!pip install --upgrade pip setuptools wheel
!pip install cudatoolkit==11.7
!pip install cuml-cu11==24.10 --index-url https://pypi.nvidia.com

# start here
from google.colab import drive
drive.mount('/content/gdrive')

!pip install -U openmim

!mim install mmcv==1.5.0

!ls /content/gdrive/My\ Drive/dinov2

!ls /content/gdrive/My\ Drive/dinov2

!ls dinov2/eval/depth/models/

# !pip uninstall torch torchvision torchaudio -y
# !pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.0 --index-url https://download.pytorch.org/whl/cu117

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
sys.path.append('/content/gdrive/My Drive/dinov2')
sys.path.append('/content/gdrive/My Drive/dinov2/dinov2')
print(os.getcwd())
print(sys.path)
# %cd /content/gdrive/My Drive/dinov2
# %cd /content/gdrive/My Drive/dinov2
# !pip install -e .

# %cd /content/gdrive/My Drive/dinov2
from dinov2.eval.depth.models.builder import build_depther

"""## Utilities"""

import math
import itertools
from functools import partial

import torch
import torch.nn.functional as F

from dinov2.eval.depth.models import build_depther


class CenterPadding(torch.nn.Module):
    def __init__(self, multiple):
        super().__init__()
        self.multiple = multiple

    def _get_pad(self, size):
        new_size = math.ceil(size / self.multiple) * self.multiple
        pad_size = new_size - size
        pad_size_left = pad_size // 2
        pad_size_right = pad_size - pad_size_left
        return pad_size_left, pad_size_right

    @torch.inference_mode()
    def forward(self, x):
        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))
        output = F.pad(x, pads)
        return output


def create_depther(cfg, backbone_model, backbone_size, head_type):
    train_cfg = cfg.get("train_cfg")
    test_cfg = cfg.get("test_cfg")
    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)

    depther.backbone.forward = partial(
        backbone_model.get_intermediate_layers,
        n=cfg.model.backbone.out_indices,
        reshape=True,
        return_class_token=cfg.model.backbone.output_cls_token,
        norm=cfg.model.backbone.final_norm,
    )
    print('length of outputs from intermediate layers', backbone_model.get_intermediate_layers)

    if hasattr(backbone_model, "patch_size"):
        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))

    return depther

"""## Load pretrained backbone"""

BACKBONE_SIZE = "small" # in ("small", "base", "large" or "giant")


backbone_archs = {
    "small": "vits14",
    "base": "vitb14",
    "large": "vitl14",
    "giant": "vitg14",
}
backbone_arch = backbone_archs[BACKBONE_SIZE]
backbone_name = f"dinov2_{backbone_arch}"

backbone_model = torch.hub.load(repo_or_dir="facebookresearch/dinov2", model=backbone_name)
backbone_model.eval()
backbone_model.cuda()

print('these are the blocks: ', backbone_model.blocks)  # List of transformer blocks
print('*'*100)
print('the normalization: ',backbone_model.norm)    # Final normalization layer
print('*'*100,'\n','\n')
print('the patch embeddings: ',backbone_model.patch_embed)  # Initial patch embedding
print('*'*100)

pip install ncut_pytorch

"""## Load pretrained depth head"""

import urllib

import mmcv
from mmcv.runner import load_checkpoint


def load_config_from_url(url: str) -> str:
    with urllib.request.urlopen(url) as f:
        return f.read().decode()


HEAD_DATASET = "nyu" # in ("nyu", "kitti")
HEAD_TYPE = "dpt" # in ("linear", "linear4", "dpt")


DINOV2_BASE_URL = "https://dl.fbaipublicfiles.com/dinov2"
head_config_url = f"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py"
head_checkpoint_url = f"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth"

cfg_str = load_config_from_url(head_config_url)
cfg = mmcv.Config.fromstring(cfg_str, file_format=".py")

model = create_depther(
    cfg,
    backbone_model=backbone_model,
    backbone_size=BACKBONE_SIZE,
    head_type=HEAD_TYPE,
)

load_checkpoint(model, head_checkpoint_url, map_location="cpu")
model.eval()
model.cuda()

"""## Load sample image"""

import urllib

from PIL import Image


def load_image_from_url(url: str) -> Image:
    with urllib.request.urlopen(url) as f:
        return Image.open(f).convert("RGB")


EXAMPLE_IMAGE_URL = "https://dl.fbaipublicfiles.com/dinov2/images/example.jpg"


image = load_image_from_url(EXAMPLE_IMAGE_URL)
display(image)

"""## Estimate depth on sample image"""

import matplotlib
from torchvision import transforms

'''Images are often represented with four channels: Red (R), Green (G), Blue   (B), and Alpha (A).
The alpha channel represents transparency or opacity for each pixel in an image. A fully transparent pixel has an alpha value of 0, while a fully opaque pixel has a value of 255.
Does it apply to depth estimation?
Depth estimation tasks typically operate on RGB (3-channel) images because the depth information relates to spatial and semantic properties derived from the visible spectrum.
'''

def make_depth_transform() -> transforms.Compose:
    return transforms.Compose([
        transforms.ToTensor(),
        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255
        transforms.Normalize(
            mean=(123.675, 116.28, 103.53),
            std=(58.395, 57.12, 57.375),
        ),
    ])


def render_depth(values, colormap_name="magma_r") -> Image:
    min_value, max_value = values.min(), values.max()
    normalized_values = (values - min_value) / (max_value - min_value)

    colormap = matplotlib.colormaps[colormap_name]
    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)
    print('colours shape', colors.shape)
    colors = colors[:, :, :3] # Discard alpha component
    return Image.fromarray(colors)


transform = make_depth_transform()

scale_factor = 1
rescaled_image = image.resize((scale_factor * image.width, scale_factor * image.height))
print('PIL image', rescaled_image.size)
transformed_image = transform(rescaled_image)
batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image

with torch.inference_mode():
    result = model.whole_inference(batch, img_meta=None, rescale=True)

depth_image = render_depth(result.squeeze().cpu())
display(depth_image)

## Visualizaing the output of certain layers
from ncut_pytorch import NCUT
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

def feature_extractor(images, resolution=(448, 448), layer=11):

    if not isinstance(images, list):
        images = [images]

    transform_v2 = transforms.Compose(
        [
            transforms.Resize(resolution),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    # extract DINOv2 last layer features from the image
    backbone_model.requires_grad_(False)
    backbone_model.eval()
    backbone_model.cuda()


    feats = []
    for i, image in enumerate(images):
        torch_image = transform_v2(image)
        feat = backbone_model(images, reshape=True, n=np.arange(12))[layer]
        # feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()
        feat = feat.squeeze(0).permute(1, 2, 0)
        feats.append(feat)
    feats = torch.stack(feats)
    return feats  # (B, H, W, D)

feat = feature_extractor(image, resolution=(480, 630), layer=9)
feat = feat.squeeze(0)
print(feat.shape)

# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# My implementation


# num_layers = 12
# # batch = F.interpolate(batch,size = (490,630), mode = 'bilinear', align_corners=False)
# # print('resized batch of image', batch.shape)

# with torch.no_grad():
#   print(batch.shape)
#   inter_features = backbone_model.get_intermediate_layers(batch, n=num_layers, reshape=True)

# layer = inter_features[5].squeeze(0) ## removing the batch dimension
# print(layer.shape)
# h,w,c = layer.permute(1,2,0).shape
# # print(h,w,c)

# nodes = layer.reshape(h*w,c)
# print('nodes shape', nodes.shape)
# ncut_model = NCUT(num_eig=9)
# eigenvectors, eigenvalues = ncut_model.fit_transform(nodes)
# print('eigenvalues shape', eigenvalues.shape)
# print('eigenvectors shape', eigenvectors.shape)


# fig, axs = plt.subplots(3, 3, figsize=(8, 6))
# i_eig = 0

# for row in range(3):  # Display the top 9 eigenvectors
#     for col in range(3):
#         ax = axs[row, col]
#         eigen_img = eigenvectors[:, i_eig].reshape(h, w)
#         eigen_img = eigen_img.cpu().numpy()
#         ax.imshow(eigen_img, cmap="coolwarm", vmin=-0.1, vmax=0.1)
#         ax.set_title(f"Eigenvector {i_eig}")
#         ax.axis("off")
#         i_eig += 1

# plt.show()

# Hook to store intermediate activations
activations = {}

def get_activation(name):
    def hook(model, input, output):
        # print('doing backbone')
        activations[name] = output
    return hook

# Register hooks for layers in the depth head
model.decode_head.conv_depth.head[0].register_forward_hook(get_activation("conv1"))
model.decode_head.conv_depth.head[2].register_forward_hook(get_activation("conv2"))

# Pass the image through the model
with torch.inference_mode():
    _ = model.whole_inference(batch, img_meta=None, rescale=True)

# Access intermediate features
conv1_features = activations["conv1"].squeeze(0)  # Remove batch dimension
conv2_features = activations["conv2"].squeeze(0)
print('the first conv features',conv1_features.shape)
print('the second conv features',conv2_features.shape)

# Apply n-cut to conv2_features (example for the second layer)
h, w, c = conv2_features.permute(1, 2, 0).shape
nodes = conv2_features.reshape(h * w, c)

ncut_model = NCUT(num_eig=9)
eigenvectors, eigenvalues = ncut_model.fit_transform(nodes)

# Visualize eigenvectors
fig, axs = plt.subplots(3, 3, figsize=(8, 6))
i_eig = 0

for row in range(3):  # Top 9 eigenvectors
    for col in range(3):
        ax = axs[row, col]
        eigen_img = eigenvectors[:, i_eig].reshape(h, w)
        eigen_img = eigen_img.cpu().numpy()
        ax.imshow(eigen_img, cmap="coolwarm", vmin=-0.1, vmax=0.1)
        ax.set_title(f"Eigenvector {i_eig}")
        ax.axis("off")
        i_eig += 1

plt.show()

backbone_model.blocks[5].register_forward_hook(get_activation("backbone_block5"))
model.decode_head.conv_depth.head[0].register_forward_hook(get_activation("depth_conv1"))
model.decode_head.conv_depth.head[2].register_forward_hook(get_activation("depth_conv2"))

activations.keys()

backbone_model.blocks[5]

backbone_model.patch_embed.num_patches
# backbone_model.patch_size

# activations["depth_conv1"][0].shape

# Function to visualize feature maps
def visualize_feature_map(feature_map, title, num_channels=5):
    print('features example', feature_map.shape)
    # patches_height = image_size[0]// patch_size
    # patches_width = image_size[1] // patch_size
    # feature_map = feature_map[0].permute(1,0)
    # print('Updated feature map shape', feature_map.shape)
    # feature_map = feature_map.reshape(-1, patches_height, patches_width)
    # print('2Updated feature map shape', feature_map.shape)

    if title.startswith('Backbone'):
      h,w = 35,45
      feature_map = feature_map[:,1:,:].squeeze(0).reshape(h,w,-1)
      feature_map = feature_map.permute(2,0,1)
      print('backbone feat map shape', feature_map.shape)
      # return

    else:
      feature_map = feature_map[0]  # First image in the batch
      print('feature map channels', feature_map.shape)
    num_channels = min(num_channels, feature_map.shape[0])
    fig, axes = plt.subplots(1, num_channels, figsize=(15, 5))
    for i in range(num_channels):
        axes[i].imshow(feature_map[i].cpu().numpy(), cmap="coolwarm")
        axes[i].axis("off")
        axes[i].set_title(f"{title} - Channel {i}", fontsize = 8)
    plt.tight_layout()
    plt.show()

# Original image
plt.imshow(image)
print('original img shape', image.size)
plt.title("Original Image")
plt.axis("off")
plt.show()

# Visualize feature maps from the backbone
visualize_feature_map(activations["backbone_block5"], "Backbone Block 5 Feature Map")

# Visualize feature maps from the depth head
visualize_feature_map(activations["depth_conv1"], "Depth Head Conv1 Feature Map")
visualize_feature_map(activations["depth_conv2"], "Depth Head Conv2 Feature Map")

activations.keys()

help(model.whole_inference)

def custom_forward(self, x):
    # Patch embeddings
    x = self.patch_embed(x)  # Shape: (batch_size, num_patches, embed_dim)

    # Add CLS token and positional embeddings
    batch_size, num_patches, _ = x.size()
    cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: (batch_size, 1, embed_dim)
    x = torch.cat((cls_tokens, x), dim=1)  # Shape: (batch_size, num_patches + 1, embed_dim)
    x = x + self.pos_embed

    # Pass through transformer blocks
    for blk in self.blocks:
        x = blk(x)

    # Apply final normalization
    x = self.norm(x)

    return x


model.backbone.forward = custom_forward.__get__(model.backbone, model.backbone.__class__)

# with torch.no_grad():
#     backbone_output = backbone_model(batch)

# print('backbone shape', backbone_output.shape )

model.backbone.forward = custom_forward.__get__(model.backbone, model.backbone.__class__)
output = model.backbone(batch)

### Code for trial

from einops import rearrange
import torch
from PIL import Image
import torchvision.transforms as transforms
from torch import nn
import numpy as np


def feature_extractor(images, resolution=(448, 448), layer=11):

    if not isinstance(images, list):
        images = [images]

    transform = transforms.Compose(
        [
            transforms.Resize(resolution),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    # extract DINOv2 last layer features from the image
    class DiNOv2Feature(torch.nn.Module):
        def __init__(self, ver="dinov2_vitb14_reg", layer=11):
            super().__init__()
            self.dinov2 = torch.hub.load("facebookresearch/dinov2", ver)
            self.dinov2.requires_grad_(False)
            self.dinov2.eval()
            self.dinov2 = self.dinov2.cuda()
            self.layer = layer

        def forward(self, x):
            out = self.dinov2.get_intermediate_layers(x, reshape=True, n=np.arange(12))[self.layer]
            return out

    feat_extractor = DiNOv2Feature(layer=layer)

    feats = []
    for i, image in enumerate(images):
        torch_image = transform(image)
        feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()
        feat = feat.squeeze(0).permute(1, 2, 0)
        feats.append(feat)
    feats = torch.stack(feats)
    return feats  # (B, H, W, D)

# batch.shape
import requests
from PIL import Image
url = "https://huzeyann.github.io/assets/img/prof_pic_old.jpg"
img = Image.open(requests.get(url, stream=True).raw)
img

feat = feature_extractor(img, resolution=(448, 448), layer=9)
feat = feat.squeeze(0)
print(feat.shape)

# visualize top 9 eigenvectors, 3 eigenvectors per row
# %%

# visualize top 9 eigenvectors, 3 eigenvectors per row
import matplotlib.pyplot as plt
from ncut_pytorch import quantile_normalize
fig, axs = plt.subplots(3, 4, figsize=(8, 6))
i_eig = 0
for i_row in range(3):
    for i_col in range(1, 4):
        ax = axs[i_row, i_col]
        ax.imshow(eigenvectors[:, i_eig].reshape(h, w).cpu().numpy(), cmap="coolwarm", vmin=-0.1, vmax=0.1)
        ax.set_title(f"eigenvalue_{i_eig} = {eigenvalues[i_eig]:.3f}")
        ax.axis("off")
        i_eig += 1
for i_row in range(3):
    ax = axs[i_row, 0]
    start, end = i_row * 3, (i_row + 1) * 3
    rgb = quantile_normalize(eigenvectors[:, start:end]).reshape(h, w, 3).cpu().numpy()
    ax.imshow(rgb)
    ax.set_title(f"eigenvectors {start}-{end-1}")
    ax.axis("off")
plt.suptitle("Top 9 eigenvectors of Ncut DiNOv2 last layer features")
plt.tight_layout()
plt.show()
# -------------------------------------------------------------
# import matplotlib.pyplot as plt
# from ncut_pytorch import quantile_normalize
# fig, axs = plt.subplots(3, 4, figsize=(13, 10))
# i_eig = 0
# for i_row in range(3):
#     for i_col in range(1, 4):
#         ax = axs[i_row, i_col]
#         ax.imshow(eigenvectors[:, i_eig].reshape(h, w).cpu().numpy(), cmap="coolwarm", vmin=-0.1, vmax=0.1)
#         ax.set_title(f"lambda_{i_eig} = {eigenvalues[i_eig]:.3f}")
#         ax.axis("off")
#         i_eig += 1
# for i_row in range(3):
#     ax = axs[i_row, 0]
#     start, end = i_row * 3, (i_row + 1) * 3
#     rgb = quantile_normalize(eigenvectors[:, start:end]).reshape(h, w, 3).cpu().numpy()
#     ax.imshow(rgb)
#     ax.set_title(f"eigenvectors {start}-{end-1}")
#     ax.axis("off")
# plt.suptitle("Top 9 eigenvectors of Ncut DiNOv2 last layer features")
# plt.tight_layout()
# plt.show()